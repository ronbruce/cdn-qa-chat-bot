{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "# Enables tracing for LangSmith or LangChain's internal operations, which could log detailed traces for debugging purposes.\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# Enables tracing for LangSmith or LangChain's internal operations, which could log detailed traces for debugging purposes.\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file.\n",
    "load_dotenv()\n",
    "\n",
    "# Load API key from environment variable.\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    log.error(\"OPENAI_API_KEY not found in environment variables.\")\n",
    "    raise ValueError(\"OPENAI_API_KEY not set in the environment.\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_agent() -> str:\n",
    "    \"\"\"Get user agent from environment variable.\"\"\"\n",
    "    env_user_agent = os.environ.get(\"USER_AGENT\")\n",
    "    if not env_user_agent:\n",
    "        log.warning(\n",
    "            \"USER_AGENT environment variable not set, \"\n",
    "            \"consider setting it to identify your requests.\"\n",
    "        )\n",
    "        return \"DefaultLangchainUserAgent\"\n",
    "    return env_user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a strainer to filter the HTML content.\n",
    "# The `bs4.SoupStrainer` is configured to retain only the elements with the classes \"post-title\", \"post-header\", and \"post-content\".\n",
    "# This focused extraction ensures that only the necessary information for LangChain processing is captured, eliminating extraneous data.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "# Initialize the WebBaseLoader.\n",
    "# This object is responsible for fetching the content of the specified webpage and applying the filtering defined by the `bs4_strainer`.\n",
    "# It allows us to retrieve only the desired sections of the webpage while setting a custom User-Agent header to mimic a standard web browser request.\n",
    "loader = WebBaseLoader( # Document loader class\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), # Source \n",
    "    bs_kwargs={\"parse_only\": bs4_strainer}, # Converts to standadize format for processing by LangChain\n",
    "    requests_kwargs={\"headers\": {\"User-Agent\": get_user_agent()}}\n",
    ")\n",
    "\n",
    "# Load the Document from the WebBaseLoader.\n",
    "# The `loader.load()` method retrieves the webpage content and applies the previously defined filtering.\n",
    "# The resulting documents are stored in the `docs` variable, which contains the extracted data ready for processing with LangChain.\n",
    "docs = loader.load()\n",
    "\n",
    "# Print information about the extracted content.\n",
    "# This snippet outputs the length of the extracted content (in characters) and displays the first 500 characters.\n",
    "# This verification step helps ensure that the extraction process was successful and that the expected data is captured.\n",
    "# print(len(docs[0].page_content))\n",
    "# print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7056}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize RecursiveCharacterTextSplitter to break down documents into manageable chunks.\n",
    "# We set chunk_size to 1000 characters, with an overlap of 200 characters to maintain context between chunks.\n",
    "# This ensures that each chunk is small enough to fit into the context window for processing (e.g., by a model like GPT).\n",
    "# The add_start_index parameter is set to True, so we can track where each chunk starts in the original document.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "\n",
    "# Split the input documents into smaller chunks for processing using the text splitter.\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Store the embedded representations of the document chunks in a Chroma vectorstore.\n",
    "# This allows for efficient similarity searches or embedded queries in downstream tasks.\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Check the length of the content in the first chunk to ensure it's within the expected chunk size.\n",
    "len(all_splits)\n",
    "\n",
    "# Get the number of chunks created from the document splits.\n",
    "len(all_splits[0].page_content)\n",
    "\n",
    "# Retrieve metadata (such as source or chunk index) from the 11th chunk to check for useful information.\n",
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
